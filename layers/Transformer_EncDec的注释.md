## Transformer_EncDec的注释
`Transformer_EncDec.py` 脚本定义了一个针对编码和解码序列的变换器模型架构，通常用于序列到序列预测、语言翻译或任何需要在输入数据中随时间关系重要的任务。以下是此脚本中定义的组件的详细说明：

### ConvLayer
该模块将具有循环填充的卷积层应用于输入数据。这通常用于处理数据，同时保持时间顺序和连通性，这对于时间序列特别有用。卷积后，应用批量归一化和 ELU 激活函数，然后进行最大池化以在保留重要特征的同时减少维度。

### EncoderLayer
一个 `EncoderLayer` 包括：
- **自注意机制**：该机制允许层权衡输入数据中不同位置的重要性。它对于捕捉无关距离的依赖性至关重要。
- **前馈神经网络**：注意力后应用，该网络包括两个卷积层，进一步转换数据。使用层归一化和丢弃来稳定和规范训练过程。

### Encoder
`Encoder` 聚合多个 `EncoderLayer` 模块和可选的卷积层（`ConvLayer`）。编码器逐序列处理输入数据，并可以应用额外的转换（如 delta 和 tau 调整），这些通常用于根据这些参数修改注意力机制的注意力得分。

### DecoderLayer
与 `EncoderLayer` 类似，但包括：
- **自注意力**：处理解码器的输入。
- **交叉注意力**：关注编码器的输出，将输入序列的信息整合到解码器的预测过程中。
  
该层还包含一个与编码器层类似的前馈网络，设计用于在注意力机制之后处理数据。

### Decoder
`Decoder` 组合多个 `DecoderLayer` 模块，如果提供，应用最终归一化，并可选一个投影层，可用于从解码器的输出生成预测。

### 使用
在典型的变换器设置中：
- **编码器** 处理输入数据并创建一组表示，这些表示封装了输入信息。
- **解码器** 然后使用这些表示以及它自己的输入（在语言模型中训练时可以是右移的输出）生成输出序列。

这个脚本对于涉及将输入序列转换为输出序列的任务至关重要，同时捕捉数据中的复杂依赖关系和关系。它具有高度的灵活性，可以适应各种基于序列的学习任务，不仅仅是语言处理，包括金融预测、信号处理等。
